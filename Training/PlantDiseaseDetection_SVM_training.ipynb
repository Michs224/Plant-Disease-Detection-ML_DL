{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import seaborn as sns\n",
    "import mahotas as mt\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Dataset to Train & Testing (Run 2 times) change fromName with augmented_1 & augmented 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "fromName = 'augmented_2'\n",
    "toName = 'augmented_2_split'\n",
    "classes = os.listdir('./' + fromName)\n",
    "names = classes\n",
    "for name in names:\n",
    "    if not os.path.exists(\"./\" + toName + \"/train/\" + name):\n",
    "        os.makedirs(\"./\" + toName + \"/train/\" + name)\n",
    "        os.makedirs(\"./\" + toName + \"/test/\" + name)\n",
    "ii = 0\n",
    "for classe in classes:\n",
    "    # Construct the directory path for the current class\n",
    "    class_dir = './' + fromName + '/' + classe\n",
    "\n",
    "    # Check if the path is a directory\n",
    "    if not os.path.isdir(class_dir):\n",
    "        # If not, skip to the next class\n",
    "        continue\n",
    "\n",
    "    tempDic = os.listdir(class_dir)\n",
    "    tempLength = int(0.8 * len(tempDic))\n",
    "    src = './' + fromName + '/' + classe\n",
    "    dist1 = './' + toName + '/train/' + classe\n",
    "    dist2 = './' + toName + '/test/' + classe\n",
    "    ii += 1\n",
    "    for i in range(len(tempDic)):\n",
    "        if tempDic[i] == '.DS_Store':\n",
    "            continue  # Skip .DS_Store files\n",
    "        if i < tempLength:\n",
    "            shutil.copy(src + '/' + tempDic[i], dist1 + '/' + tempDic[i])\n",
    "        else:\n",
    "            shutil.copy(src + '/' + tempDic[i], dist2 + '/' + tempDic[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATASET_AUG_1_TRAIN = \"./augmented_1_split/train\"\n",
    "PATH_DATASET_AUG_1_TEST = \"./augmented_1_split/test\"\n",
    "PATH_DATASET_AUG_2_TRAIN = \"./augmented_2_split/train\"\n",
    "PATH_DATASET_AUG_2_TEST = \"./augmented_2_split/test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BGR to RGB Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting each image to RGB from BGR format\n",
    "\n",
    "def rgb_bgr(image):\n",
    "    rgb_img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return rgb_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RGB to HSV(Hue Saturation Value) Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion to HSV image format from RGB\n",
    "\n",
    "def bgr_hsv(rgb_img):\n",
    "    hsv_img = cv2.cvtColor(rgb_img, cv2.COLOR_RGB2HSV)\n",
    "    return hsv_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for extraction of green and brown color\n",
    "\n",
    "def img_segmentation(rgb_img,hsv_img):\n",
    "    lower_green = np.array([25,0,20])\n",
    "    upper_green = np.array([100,255,255])\n",
    "    healthy_mask = cv2.inRange(hsv_img, lower_green, upper_green)\n",
    "    result = cv2.bitwise_and(rgb_img,rgb_img, mask=healthy_mask)\n",
    "    lower_brown = np.array([10,0,10])\n",
    "    upper_brown = np.array([30,255,255])\n",
    "    disease_mask = cv2.inRange(hsv_img, lower_brown, upper_brown)\n",
    "    disease_result = cv2.bitwise_and(rgb_img, rgb_img, mask=disease_mask)\n",
    "    final_mask = healthy_mask + disease_mask\n",
    "    final_result = cv2.bitwise_and(rgb_img, rgb_img, mask=final_mask)\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Descriptor:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Hu Moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature-descriptor-1: Hu Moments\n",
    "def fd_hu_moments(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    feature = cv2.HuMoments(cv2.moments(image)).flatten()\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Haralick Texture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature-descriptor-2: Haralick Texture\n",
    "def fd_haralick(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    haralick = mt.features.haralick(gray).mean(axis=0)\n",
    "    return haralick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature-descriptor-3: Color Histogram\n",
    "def fd_histogram(image, mask=None):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    hist  = cv2.calcHist([image], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n",
    "    cv2.normalize(hist, hist)\n",
    "    return hist.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading up the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple___Apple_scab', 'Apple___Black_rot', 'Apple___Cedar_apple_rust', 'Apple___healthy', 'Blueberry___healthy', 'Cherry_(including_sour)___Powdery_mildew', 'Cherry_(including_sour)___healthy', 'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot', 'Corn_(maize)___Common_rust_', 'Corn_(maize)___Northern_Leaf_Blight', 'Corn_(maize)___healthy', 'Grape___Black_rot', 'Grape___Esca_(Black_Measles)', 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)', 'Grape___healthy', 'Orange___Haunglongbing_(Citrus_greening)', 'Peach___Bacterial_spot', 'Peach___healthy', 'Pepper,_bell___Bacterial_spot', 'Pepper,_bell___healthy', 'Potato___Early_blight', 'Potato___Late_blight', 'Potato___healthy', 'Raspberry___healthy', 'Soybean___healthy', 'Squash___Powdery_mildew', 'Strawberry___Leaf_scorch', 'Strawberry___healthy', 'Tomato___Bacterial_spot', 'Tomato___Early_blight', 'Tomato___Late_blight', 'Tomato___Leaf_Mold', 'Tomato___Septoria_leaf_spot', 'Tomato___Spider_mites Two-spotted_spider_mite', 'Tomato___Target_Spot', 'Tomato___Tomato_Yellow_Leaf_Curl_Virus', 'Tomato___Tomato_mosaic_virus', 'Tomato___healthy']\n",
      "['Apple___Apple_scab', 'Apple___Black_rot', 'Apple___Cedar_apple_rust', 'Apple___healthy', 'Blueberry___healthy', 'Cherry_(including_sour)___Powdery_mildew', 'Cherry_(including_sour)___healthy', 'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot', 'Corn_(maize)___Common_rust_', 'Corn_(maize)___Northern_Leaf_Blight', 'Corn_(maize)___healthy', 'Grape___Black_rot', 'Grape___Esca_(Black_Measles)', 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)', 'Grape___healthy', 'Orange___Haunglongbing_(Citrus_greening)', 'Peach___Bacterial_spot', 'Peach___healthy', 'Pepper,_bell___Bacterial_spot', 'Pepper,_bell___healthy', 'Potato___Early_blight', 'Potato___Late_blight', 'Potato___healthy', 'Raspberry___healthy', 'Soybean___healthy', 'Squash___Powdery_mildew', 'Strawberry___Leaf_scorch', 'Strawberry___healthy', 'Tomato___Bacterial_spot', 'Tomato___Early_blight', 'Tomato___Late_blight', 'Tomato___Leaf_Mold', 'Tomato___Septoria_leaf_spot', 'Tomato___Spider_mites Two-spotted_spider_mite', 'Tomato___Target_Spot', 'Tomato___Tomato_Yellow_Leaf_Curl_Virus', 'Tomato___Tomato_mosaic_virus', 'Tomato___healthy']\n"
     ]
    }
   ],
   "source": [
    "# get the training labels\n",
    "train_labels_1 = []\n",
    "train_temp_1 = os.listdir(PATH_DATASET_AUG_1_TRAIN)\n",
    "for i in range(len(train_temp_1)):\n",
    "    if train_temp_1[i] == '.DS_Store':\n",
    "        continue  # Skip .DS_Store files\n",
    "    train_labels_1.append(train_temp_1[i])\n",
    "    \n",
    "train_labels_2 = []\n",
    "train_temp_2 = os.listdir(PATH_DATASET_AUG_2_TRAIN)\n",
    "for i in range(len(train_temp_2)):\n",
    "    if train_temp_2[i] == '.DS_Store':\n",
    "        continue  # Skip .DS_Store files\n",
    "    train_labels_2.append(train_temp_1[i])\n",
    "    \n",
    "# sort the training labels\n",
    "train_labels_1.sort()\n",
    "print(train_labels_1)\n",
    "\n",
    "train_labels_2.sort()\n",
    "print(train_labels_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating the Features and Label Embeddings from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STATUS] Starting Global Feature Extraction for label set 1...\n",
      "[STATUS] Processed folder: Apple___Apple_scab\n",
      "[STATUS] Processed folder: Apple___Black_rot\n",
      "[STATUS] Processed folder: Apple___Cedar_apple_rust\n",
      "[STATUS] Processed folder: Apple___healthy\n",
      "[STATUS] Processed folder: Blueberry___healthy\n",
      "[STATUS] Processed folder: Cherry_(including_sour)___Powdery_mildew\n",
      "[STATUS] Processed folder: Cherry_(including_sour)___healthy\n",
      "[STATUS] Processed folder: Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot\n",
      "[STATUS] Processed folder: Corn_(maize)___Common_rust_\n",
      "[STATUS] Processed folder: Corn_(maize)___Northern_Leaf_Blight\n",
      "[STATUS] Processed folder: Corn_(maize)___healthy\n",
      "[STATUS] Processed folder: Grape___Black_rot\n",
      "[STATUS] Processed folder: Grape___Esca_(Black_Measles)\n",
      "[STATUS] Processed folder: Grape___Leaf_blight_(Isariopsis_Leaf_Spot)\n",
      "[STATUS] Processed folder: Grape___healthy\n",
      "[STATUS] Processed folder: Orange___Haunglongbing_(Citrus_greening)\n",
      "[STATUS] Processed folder: Peach___Bacterial_spot\n",
      "[STATUS] Processed folder: Peach___healthy\n",
      "[STATUS] Processed folder: Pepper,_bell___Bacterial_spot\n",
      "[STATUS] Processed folder: Pepper,_bell___healthy\n",
      "[STATUS] Processed folder: Potato___Early_blight\n",
      "[STATUS] Processed folder: Potato___Late_blight\n",
      "[STATUS] Processed folder: Potato___healthy\n",
      "[STATUS] Processed folder: Raspberry___healthy\n",
      "[STATUS] Processed folder: Soybean___healthy\n",
      "[STATUS] Processed folder: Squash___Powdery_mildew\n",
      "[STATUS] Processed folder: Strawberry___Leaf_scorch\n",
      "[STATUS] Processed folder: Strawberry___healthy\n",
      "[STATUS] Processed folder: Tomato___Bacterial_spot\n",
      "[STATUS] Processed folder: Tomato___Early_blight\n",
      "[STATUS] Processed folder: Tomato___Late_blight\n",
      "[STATUS] Processed folder: Tomato___Leaf_Mold\n",
      "[STATUS] Processed folder: Tomato___Septoria_leaf_spot\n",
      "[STATUS] Processed folder: Tomato___Spider_mites Two-spotted_spider_mite\n",
      "[STATUS] Processed folder: Tomato___Target_Spot\n",
      "[STATUS] Processed folder: Tomato___Tomato_Yellow_Leaf_Curl_Virus\n",
      "[STATUS] Processed folder: Tomato___Tomato_mosaic_virus\n",
      "[STATUS] Processed folder: Tomato___healthy\n",
      "[STATUS] Starting Global Feature Extraction for label set 2...\n",
      "[STATUS] Processed folder: Apple___Apple_scab\n",
      "[STATUS] Processed folder: Apple___Black_rot\n",
      "[STATUS] Processed folder: Apple___Cedar_apple_rust\n",
      "[STATUS] Processed folder: Apple___healthy\n",
      "[STATUS] Processed folder: Blueberry___healthy\n",
      "[STATUS] Processed folder: Cherry_(including_sour)___Powdery_mildew\n",
      "[STATUS] Processed folder: Cherry_(including_sour)___healthy\n",
      "[STATUS] Processed folder: Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot\n",
      "[STATUS] Processed folder: Corn_(maize)___Common_rust_\n",
      "[STATUS] Processed folder: Corn_(maize)___Northern_Leaf_Blight\n",
      "[STATUS] Processed folder: Corn_(maize)___healthy\n",
      "[STATUS] Processed folder: Grape___Black_rot\n",
      "[STATUS] Processed folder: Grape___Esca_(Black_Measles)\n",
      "[STATUS] Processed folder: Grape___Leaf_blight_(Isariopsis_Leaf_Spot)\n",
      "[STATUS] Processed folder: Grape___healthy\n",
      "[STATUS] Processed folder: Orange___Haunglongbing_(Citrus_greening)\n",
      "[STATUS] Processed folder: Peach___Bacterial_spot\n",
      "[STATUS] Processed folder: Peach___healthy\n",
      "[STATUS] Processed folder: Pepper,_bell___Bacterial_spot\n",
      "[STATUS] Processed folder: Pepper,_bell___healthy\n",
      "[STATUS] Processed folder: Potato___Early_blight\n",
      "[STATUS] Processed folder: Potato___Late_blight\n",
      "[STATUS] Processed folder: Potato___healthy\n",
      "[STATUS] Processed folder: Raspberry___healthy\n",
      "[STATUS] Processed folder: Soybean___healthy\n",
      "[STATUS] Processed folder: Squash___Powdery_mildew\n",
      "[STATUS] Processed folder: Strawberry___Leaf_scorch\n",
      "[STATUS] Processed folder: Strawberry___healthy\n",
      "[STATUS] Processed folder: Tomato___Bacterial_spot\n",
      "[STATUS] Processed folder: Tomato___Early_blight\n",
      "[STATUS] Processed folder: Tomato___Late_blight\n",
      "[STATUS] Processed folder: Tomato___Leaf_Mold\n",
      "[STATUS] Processed folder: Tomato___Septoria_leaf_spot\n",
      "[STATUS] Processed folder: Tomato___Spider_mites Two-spotted_spider_mite\n",
      "[STATUS] Processed folder: Tomato___Target_Spot\n",
      "[STATUS] Processed folder: Tomato___Tomato_Yellow_Leaf_Curl_Virus\n",
      "[STATUS] Processed folder: Tomato___Tomato_mosaic_virus\n",
      "[STATUS] Processed folder: Tomato___healthy\n",
      "[STATUS] Completed Global Feature Extraction for both label sets...\n",
      "Label set 1 has 31539 labels and 31539 global features.\n",
      "Label set 2 has 31539 labels and 31539 global features.\n"
     ]
    }
   ],
   "source": [
    "fixed_size = (500, 500)\n",
    "\n",
    "# Combine both label sets and prepare separate storage\n",
    "train_label_sets = [train_labels_1, train_labels_2]\n",
    "\n",
    "# Separate storage for labels and global features \n",
    "# Augmented 1\n",
    "labels_1 = []\n",
    "global_features_1 = []\n",
    "\n",
    "# Augmented 2\n",
    "labels_2 = []\n",
    "global_features_2 = []\n",
    "\n",
    "# Loop over both training label sets\n",
    "for i, train_labels in enumerate(train_label_sets):\n",
    "    print(f\"[STATUS] Starting Global Feature Extraction for label set {i+1}...\")\n",
    "\n",
    "    # Use separate storage based on index\n",
    "    if i == 0:\n",
    "        current_sets = train_labels_1\n",
    "        current_labels = labels_1\n",
    "        current_global_features = global_features_1\n",
    "        dataset_path = PATH_DATASET_AUG_1_TRAIN\n",
    "    else:\n",
    "        current_sets = train_labels_2\n",
    "        current_labels = labels_2\n",
    "        current_global_features = global_features_2\n",
    "        dataset_path = PATH_DATASET_AUG_2_TRAIN\n",
    "\n",
    "    # Loop through the training data sub-folders\n",
    "    for training_name in train_labels:\n",
    "        # Join the training data path and each species training folder\n",
    "        img_dir_path = os.path.join(dataset_path, training_name)\n",
    "\n",
    "        # Get the current training label\n",
    "        current_label = training_name\n",
    "\n",
    "        # Loop over the images in each sub-folder\n",
    "        for img in os.listdir(img_dir_path):\n",
    "            # Get the image file name\n",
    "            file = os.path.join(img_dir_path, img)\n",
    "\n",
    "            # Read the image and resize it to a fixed size\n",
    "            image = cv2.imread(file)\n",
    "            image = cv2.resize(image, fixed_size)\n",
    "\n",
    "            # Running Function Bit By Bit\n",
    "            RGB_BGR = rgb_bgr(image)\n",
    "            BGR_HSV = bgr_hsv(RGB_BGR)\n",
    "            IMG_SEGMENT = img_segmentation(RGB_BGR, BGR_HSV)\n",
    "\n",
    "            # Call for Global Feature Descriptors\n",
    "            fv_hu_moments = fd_hu_moments(IMG_SEGMENT)\n",
    "            fv_haralick = fd_haralick(IMG_SEGMENT)\n",
    "            fv_histogram = fd_histogram(IMG_SEGMENT)\n",
    "\n",
    "            # Concatenate global features\n",
    "            global_feature = np.hstack([fv_histogram, fv_haralick, fv_hu_moments])\n",
    "\n",
    "            # Update the list of labels and feature vectors\n",
    "            current_labels.append(current_label)\n",
    "            current_global_features.append(global_feature)\n",
    "\n",
    "        print(\"[STATUS] Processed folder: {}\".format(current_label))\n",
    "\n",
    "print(\"[STATUS] Completed Global Feature Extraction for both label sets...\")\n",
    "\n",
    "# Final Results\n",
    "print(\"Label set 1 has {} labels and {} global features.\".format(len(labels_1), len(global_features_1)))\n",
    "print(\"Label set 2 has {} labels and {} global features.\".format(len(labels_2), len(global_features_2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STATUS] feature Augmented 1 vector size (31539, 532)\n",
      "[STATUS] feature Augmented 2 vector size (31539, 532)\n"
     ]
    }
   ],
   "source": [
    "# get the overall feature vector size\n",
    "print(\"[STATUS] feature Augmented 1 vector size {}\".format(np.array(global_features_1).shape))\n",
    "print(\"[STATUS] feature Augmented 2 vector size {}\".format(np.array(global_features_2).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STATUS] training Labels Augmented 1 (31539,)\n",
      "[STATUS] training Labels Augmented 2 (31539,)\n"
     ]
    }
   ],
   "source": [
    "# get the overall training label size\n",
    "# print(labels)\n",
    "print(\"[STATUS] training Labels Augmented 1 {}\".format(np.array(labels_1).shape))\n",
    "print(\"[STATUS] training Labels Augmented 2 {}\".format(np.array(labels_2).shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Label                                  | Encoded Value |\n",
    "|----------------------------------------|---------------|\n",
    "| Apple___Apple_scab                     | 0             |\n",
    "| Apple___Black_rot                      | 1             |\n",
    "| Apple___Cedar_apple_rust               | 2             |\n",
    "| Apple___healthy                        | 3             |\n",
    "| Blueberry___healthy                    | 4             |\n",
    "| Cherry_(including_sour)___healthy      | 5             |\n",
    "| Cherry_(including_sour)___Powdery_mildew | 6           |\n",
    "| Corn_(maize)___Cercospora_leaf_spot_Gray_leaf_spot | 7 |\n",
    "| Corn_(maize)___Common_rust_            | 8             |\n",
    "| Corn_(maize)___healthy                 | 9             |\n",
    "| Corn_(maize)___Northern_Leaf_Blight    | 10            |\n",
    "| Grape___Black_rot                      | 11            |\n",
    "| Grape___Esca_(Black_Measles)           | 12            |\n",
    "| Grape___healthy                        | 13            |\n",
    "| Grape___Leaf_blight_(Isariopsis_Leaf_Spot) | 14         |\n",
    "| Orange___Haunglongbing_(Citrus_greening) | 15          |\n",
    "| Peach___Bacterial_spot                 | 16            |\n",
    "| Peach___healthy                        | 17            |\n",
    "| Pepper,_bell___Bacterial_spot          | 18            |\n",
    "| Pepper,_bell___healthy                 | 19            |\n",
    "| Potato___Early_blight                  | 20            |\n",
    "| Potato___healthy                       | 21            |\n",
    "| Potato___Late_blight                   | 22            |\n",
    "| Raspberry___healthy                    | 23            |\n",
    "| Soybean___healthy                      | 24            |\n",
    "| Squash___Powdery_mildew                | 25            |\n",
    "| Strawberry___healthy                   | 26            |\n",
    "| Strawberry___Leaf_scorch               | 27            |\n",
    "| Tomato___Bacterial_spot                | 28            |\n",
    "| Tomato___Early_blight                  | 29            |\n",
    "| Tomato___healthy                       | 30            |\n",
    "| Tomato___Late_blight                   | 31            |\n",
    "| Tomato___Leaf_Mold                     | 32            |\n",
    "| Tomato___Septoria_leaf_spot            | 33            |\n",
    "| Tomato___Spider_mites_Two-spotted_spider_mite | 34     |\n",
    "| Tomato___Target_Spot                   | 35            |\n",
    "| Tomato___Tomato_mosaic_virus           | 36            |\n",
    "| Tomato___Tomato_Yellow_Leaf_Curl_Virus | 37            |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate tuple (not \"int\") to tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# encode the target labels\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(label_sets):\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[STATUS] Starting Encode for label set \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Use separate storage based on index\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate tuple (not \"int\") to tuple"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_sets = [labels_1, labels_2]\n",
    "# encode the target labels\n",
    "for i in enumerate(label_sets):\n",
    "    print(f\"[STATUS] Starting Encode for label set {i+1}...\")\n",
    "\n",
    "    # Use separate storage based on index\n",
    "    if i == 0:\n",
    "        targetNames_1 = np.unique(labels_1)\n",
    "        le_1 = LabelEncoder()\n",
    "        target_1 = le_1.fit_transform(labels_1)\n",
    "        print(targetNames_1)\n",
    "        print(\"[STATUS] training labels 1 encoded...\")\n",
    "    else:\n",
    "        targetNames_2 = np.unique(labels_2)\n",
    "        le_2 = LabelEncoder()\n",
    "        target_2 = le_2.fit_transform(labels_2)\n",
    "        print(targetNames_2)\n",
    "        print(\"[STATUS] training labels 2 encoded...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling using MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale features in the range (0-1)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "global_features_sets = [global_features_1, global_features_2]\n",
    "for i in enumerate(train_label_sets):\n",
    "    print(f\"[STATUS] Starting feature vector Normalization for global feature {i+1}...\")\n",
    "\n",
    "    # Use separate storage based on index\n",
    "    if i == 0:\n",
    "        scaler_1 = MinMaxScaler(feature_range=(0, 1))\n",
    "        rescaled_features_1 = scaler_1.fit_transform(global_features_1)\n",
    "        print(\"[STATUS] feature vector 1 normalized...\")\n",
    "        rescaled_features_1\n",
    "    else:\n",
    "        scaler_2 = MinMaxScaler(feature_range=(0, 1))\n",
    "        rescaled_features_2 = scaler_2.fit_transform(global_features_2)\n",
    "        print(\"[STATUS] feature vector 2 normalized...\")\n",
    "        rescaled_features_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmented 1\n",
    "print(\"[STATUS] target labels 1: {}\".format(target_1))\n",
    "print(\"[STATUS] target labels 1 shape: {}\".format(target_1.shape))\n",
    "\n",
    "# Augmented 2\n",
    "print(\"[STATUS] target labels 2: {}\".format(target_2))\n",
    "print(\"[STATUS] target labels 2 shape: {}\".format(target_2.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Features and Labels Embeddings in h5py format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmented 1\n",
    "# save the feature vector 1 using HDF5\n",
    "h5f_data = h5py.File('../embeddings/features/features_1.h5', \"w\")\n",
    "h5f_data.create_dataset(\"dataset_1\", data=np.array(rescaled_features_1))\n",
    "\n",
    "# save the label vector 1 using HDF5\n",
    "h5f_label = h5py.File('../embeddings/labels/labels_1.h5', \"w\")\n",
    "h5f_label.create_dataset(\"dataset_1\", data=np.array(target_1))\n",
    "\n",
    "# Augmented 2\n",
    "# save the feature vector 2 using HDF5\n",
    "h5f_data = h5py.File('../embeddings/features/features_2.h5', \"w\")\n",
    "h5f_data.create_dataset(\"dataset_2\", data=np.array(rescaled_features_2))\n",
    "\n",
    "# save the label vector 2 using HDF5\n",
    "h5f_label = h5py.File('../embeddings/labels/labels_2.h5', \"w\")\n",
    "h5f_label.create_dataset(\"dataset_2\", data=np.array(target_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f_data.close()\n",
    "h5f_label.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the different models and calculating the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Loading the Features and Labels Embeddings from the h5py format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmented 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "num_trees = 100\n",
    "test_size = 0.20\n",
    "seed = 9\n",
    "scoring = \"accuracy\"\n",
    "\n",
    "# get the training labels\n",
    "train_labels_1 = os.listdir(PATH_DATASET_AUG_1_TRAIN)\n",
    "\n",
    "\n",
    "# sort the training labels\n",
    "train_labels_1.sort()\n",
    "\n",
    "# create all the machine learning models\n",
    "models = []\n",
    "models.append((\"SVM\", SVC(random_state=seed)))\n",
    "\n",
    "# variables to hold the results and names\n",
    "# Augmented 1\n",
    "results_1 = []\n",
    "names_1 = []\n",
    "\n",
    "# import the feature vector and trained labels\n",
    "# Augemented 1\n",
    "h5f_data_1 = h5py.File('../embeddings/features/features_1.h5', \"r\")\n",
    "h5f_label_1 = h5py.File('../embeddings/labels/labels_1.h5', \"r\")\n",
    "\n",
    "\n",
    "global_features_string_1 = h5f_data_1[\"dataset_1\"]\n",
    "global_labels_string_1 = h5f_label_1[\"dataset_1\"]\n",
    "\n",
    "global_features_1 = np.array(global_features_string_1)\n",
    "global_labels_1 = np.array(global_labels_string_1)\n",
    "\n",
    "h5f_data_1.close()\n",
    "h5f_label_1.close()\n",
    "\n",
    "# verify the shape of the feature vector and labels\n",
    "print(\"[STATUS] features shape: {}\".format(global_features_1.shape))\n",
    "print(\"[STATUS] labels shape: {}\".format(global_labels_1.shape))\n",
    "\n",
    "print(\"[STATUS] training started...\")\n",
    "print(global_labels_1, len(global_labels_1), len(global_features_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmented 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the training labels\n",
    "train_labels_2 = os.listdir(PATH_DATASET_AUG_2_TRAIN)\n",
    "\n",
    "\n",
    "# sort the training labels\n",
    "train_labels_2.sort()\n",
    "\n",
    "# create all the machine learning models\n",
    "models = []\n",
    "models.append((\"SVM\", SVC(random_state=seed)))\n",
    "\n",
    "# variables to hold the results and names\n",
    "# Augmented 1\n",
    "results_2 = []\n",
    "names_2 = []\n",
    "\n",
    "# import the feature vector and trained labels\n",
    "# Augemented 1\n",
    "h5f_data_2 = h5py.File('../embeddings/features/features_2.h5', \"r\")\n",
    "h5f_label_2 = h5py.File('../embeddings/labels/labels_2.h5', \"r\")\n",
    "\n",
    "\n",
    "global_features_string_2 = h5f_data_2[\"dataset_2\"]\n",
    "global_labels_string_2 = h5f_label_2[\"dataset_2\"]\n",
    "\n",
    "global_features_2 = np.array(global_features_string_2)\n",
    "global_labels_2 = np.array(global_labels_string_2)\n",
    "\n",
    "h5f_data_2.close()\n",
    "h5f_label_2.close()\n",
    "\n",
    "# verify the shape of the feature vector and labels\n",
    "print(\"[STATUS] features shape: {}\".format(global_features_2.shape))\n",
    "print(\"[STATUS] labels shape: {}\".format(global_labels_2.shape))\n",
    "\n",
    "print(\"[STATUS] training started...\")\n",
    "print(global_labels_2, len(global_labels_2), len(global_features_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmented 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the training and testing data\n",
    "(\n",
    "    trainDataGlobal_1,\n",
    "    testDataGlobal_1,\n",
    "    trainLabelsGlobal_1,\n",
    "    testLabelsGlobal_1,\n",
    ") = train_test_split(np.array(global_features_1), np.array(global_labels_1), test_size=test_size, random_state=seed)\n",
    "\n",
    "print(\"[STATUS] splitted train and test data...\")\n",
    "print(\"Train data  : {}\".format(trainDataGlobal_1.shape))\n",
    "print(\"Test data   : {}\".format(testDataGlobal_1.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmented 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the training and testing data\n",
    "(\n",
    "    trainDataGlobal_2,\n",
    "    testDataGlobal_2,\n",
    "    trainLabelsGlobal_2,\n",
    "    testLabelsGlobal_2,\n",
    ") = train_test_split(np.array(global_features_2), np.array(global_labels_2), test_size=test_size, random_state=seed)\n",
    "\n",
    "print(\"[STATUS] splitted train and test data...\")\n",
    "print(\"Train data  : {}\".format(trainDataGlobal_2.shape))\n",
    "print(\"Test data   : {}\".format(testDataGlobal_2.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataGlobal_1 \n",
    "trainDataGlobal_2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation using Augmented 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-fold cross validation\n",
    "for name_1, model in models:\n",
    "    kfold = KFold(n_splits=10)\n",
    "    cv_results_1 = cross_val_score(\n",
    "        model, trainDataGlobal_1, trainLabelsGlobal_1, cv=kfold, scoring=scoring\n",
    "    )\n",
    "    results_1.append(cv_results_1)\n",
    "    names_1.append(name_1)\n",
    "    msg = \"%s: %f (%f)\" % (name_1, cv_results_1.mean(), cv_results_1.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation using Augmented 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-fold cross validation\n",
    "for name_2, model in models:\n",
    "    kfold = KFold(n_splits=10)\n",
    "    cv_results_2 = cross_val_score(\n",
    "        model, trainDataGlobal_2, trainLabelsGlobal_2, cv=kfold, scoring=scoring\n",
    "    )\n",
    "    results_2.append(cv_results_2)\n",
    "    names_2.append(name_2)\n",
    "    msg = \"%s: %f (%f)\" % (name_2, cv_results_2.mean(), cv_results_2.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot algorithm\n",
    "fig = plt.figure()\n",
    "fig.suptitle(\"SVM with Augmented 1\")\n",
    "ax = fig.add_subplot(2)\n",
    "plt.boxplot(results_1)\n",
    "ax.set_xticklabels(names_1)\n",
    "\n",
    "\n",
    "# boxplot algorithm\n",
    "fig = plt.figure()\n",
    "fig.suptitle(\"SVM with Augmented 2\")\n",
    "ax = fig.add_subplot(2)\n",
    "plt.boxplot(results_2)\n",
    "ax.set_xticklabels(names_2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifying the accuracy for the SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmented 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.fit(trainDataGlobal_1, trainLabelsGlobal_1)\n",
    "len(trainDataGlobal_1), len(trainLabelsGlobal_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_1 = svm.predict(testDataGlobal_1)\n",
    "testLabelsGlobal_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmented 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.fit(trainDataGlobal_2, trainLabelsGlobal_2)\n",
    "len(trainDataGlobal_2), len(trainLabelsGlobal_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_2 = svm.predict(testDataGlobal_2)\n",
    "testLabelsGlobal_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, labels, title):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Confusion Matrix - {title}', fontsize=16)\n",
    "    plt.show()\n",
    "    \n",
    "plot_confusion_matrix(testLabelsGlobal_1, y_predict_1, labels_1, 'Augmented 1')\n",
    "plot_confusion_matrix(testLabelsGlobal_2, y_predict_2, labels_2, 'Augmented 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(testLabelsGlobal_1, y_predict_1))\n",
    "\n",
    "print(classification_report(testLabelsGlobal_2, y_predict_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(f\"Accuracy SVM augmented 1 : {accuracy_score(testLabelsGlobal_1, y_predict_1)}\")\n",
    "\n",
    "print(f\"Accuracy SVM augmented 2 : {accuracy_score(testLabelsGlobal_2, y_predict_2)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep_L",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
